{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n# data manipulation, gathering\nimport numpy as np # for array manipulation\nimport pandas as pd # for dataframe manipulation/reading in data\nimport json # for reading in Data\nfrom itertools import islice # for slicing and dicing JSON records\nimport os # for getting the filepath information\nimport re # to identify characters that are to be removed\nimport nltk # for preprocessing of textual data\nfrom nltk.corpus import stopwords # for removing stopwords\nfrom nltk.tokenize import word_tokenize # for tokenizing text\nfrom nltk.stem import WordNetLemmatizer # for lemmatizing text\nfrom sklearn.feature_extraction.text import TfidfVectorizer # for featurizing text\nfrom sklearn.metrics.pairwise import cosine_similarity # for getting similarity score","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:41.271653Z","iopub.execute_input":"2023-03-15T11:01:41.272051Z","iopub.status.idle":"2023-03-15T11:01:42.996672Z","shell.execute_reply.started":"2023-03-15T11:01:41.272013Z","shell.execute_reply":"2023-03-15T11:01:42.995565Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:42.999223Z","iopub.execute_input":"2023-03-15T11:01:43.000567Z","iopub.status.idle":"2023-03-15T11:01:43.180583Z","shell.execute_reply.started":"2023-03-15T11:01:43.000511Z","shell.execute_reply":"2023-03-15T11:01:43.179191Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:43.182248Z","iopub.execute_input":"2023-03-15T11:01:43.183201Z","iopub.status.idle":"2023-03-15T11:01:44.635622Z","shell.execute_reply.started":"2023-03-15T11:01:43.183149Z","shell.execute_reply":"2023-03-15T11:01:44.634582Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing the dataset","metadata":{}},{"cell_type":"code","source":"#Function to yield data from the stored file\ndef extract_data(path):\n    with open(path, 'r') as f:\n        for x in f:\n            yield x\n            \n#Defining PATH\nPATH = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\n\n\n#Creating a data generator to extract data from the JSON file\ndata_gen = extract_data(PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-15T11:01:44.638276Z","iopub.execute_input":"2023-03-15T11:01:44.638891Z","iopub.status.idle":"2023-03-15T11:01:44.646131Z","shell.execute_reply.started":"2023-03-15T11:01:44.638851Z","shell.execute_reply":"2023-03-15T11:01:44.644854Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Function to yield N records from the data generator\ndef fetch_n_records(data_gen, n):\n    return [json.loads(record) for record in islice(data_gen, n)]\n\n#Fetching 250000 records from the given data for the use of recommender systems\nCHUNK_SIZE = 250000\ndata = fetch_n_records(data_gen, CHUNK_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:44.648162Z","iopub.execute_input":"2023-03-15T11:01:44.648738Z","iopub.status.idle":"2023-03-15T11:01:55.969720Z","shell.execute_reply.started":"2023-03-15T11:01:44.648692Z","shell.execute_reply":"2023-03-15T11:01:55.968403Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#The variable data records is a list of dictionaries\ndata[77]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:55.971203Z","iopub.execute_input":"2023-03-15T11:01:55.971641Z","iopub.status.idle":"2023-03-15T11:01:55.980009Z","shell.execute_reply.started":"2023-03-15T11:01:55.971601Z","shell.execute_reply":"2023-03-15T11:01:55.978714Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': '0704.0078',\n 'submitter': 'Raul Vera',\n 'authors': 'Marc Mars, Filipe C. Mena, Raul Vera',\n 'title': 'Linear perturbations of matched spacetimes: the gauge problem and\\n  background symmetries',\n 'comments': '18 pages, plain LaTeX file',\n 'journal-ref': 'Class.Quant.Grav.24:3673-3690,2007',\n 'doi': '10.1088/0264-9381/24/14/008',\n 'report-no': None,\n 'categories': 'gr-qc',\n 'license': None,\n 'abstract': '  We present a critical review about the study of linear perturbations of\\nmatched spacetimes including gauge problems. We analyse the freedom introduced\\nin the perturbed matching by the presence of background symmetries and revisit\\nthe particular case of spherically symmetry in n-dimensions. This analysis\\nincludes settings with boundary layers such as brane world models and shell\\ncosmologies.\\n',\n 'versions': [{'version': 'v1', 'created': 'Sun, 1 Apr 2007 10:08:25 GMT'}],\n 'update_date': '2008-11-26',\n 'authors_parsed': [['Mars', 'Marc', ''],\n  ['Mena', 'Filipe C.', ''],\n  ['Vera', 'Raul', '']]}"},"metadata":{}}]},{"cell_type":"code","source":"# #Resampling method to fetch records for generating user_profile and recommendation algorithm\n# def split_records(data_records, profile_capacity=100, random_state=42):\n#     np.random.seed(random_state)\n#     np.random.shuffle(data_records)\n#     profile_records, recommend_records = data_records[:profile_capacity], data_records[profile_capacity:]\n#     return profile_records, recommend_records\n\n# #Splitting the fetched records into profile and recommendation records\n# profile_records, recommend_records = split_records(data_records, profile_capacity=500)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:55.981342Z","iopub.execute_input":"2023-03-15T11:01:55.981815Z","iopub.status.idle":"2023-03-15T11:01:55.992814Z","shell.execute_reply.started":"2023-03-15T11:01:55.981780Z","shell.execute_reply":"2023-03-15T11:01:55.991343Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Function to generate a dataframe from a list of dictionaries\ndef get_dataframe(list_of_dicts):\n    data = pd.DataFrame(list_of_dicts)\n    return data\n\n#Generating dataframe from the list of records\ndata_df = get_dataframe(data)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:55.994938Z","iopub.execute_input":"2023-03-15T11:01:55.995393Z","iopub.status.idle":"2023-03-15T11:01:56.872736Z","shell.execute_reply.started":"2023-03-15T11:01:55.995355Z","shell.execute_reply":"2023-03-15T11:01:56.871565Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:56.874593Z","iopub.execute_input":"2023-03-15T11:01:56.875026Z","iopub.status.idle":"2023-03-15T11:01:57.406960Z","shell.execute_reply.started":"2023-03-15T11:01:56.874980Z","shell.execute_reply":"2023-03-15T11:01:57.405589Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 250000 entries, 0 to 249999\nData columns (total 14 columns):\n #   Column          Non-Null Count   Dtype \n---  ------          --------------   ----- \n 0   id              250000 non-null  object\n 1   submitter       250000 non-null  object\n 2   authors         250000 non-null  object\n 3   title           250000 non-null  object\n 4   comments        213517 non-null  object\n 5   journal-ref     123099 non-null  object\n 6   doi             151332 non-null  object\n 7   report-no       21668 non-null   object\n 8   categories      250000 non-null  object\n 9   license         206768 non-null  object\n 10  abstract        250000 non-null  object\n 11  versions        250000 non-null  object\n 12  update_date     250000 non-null  object\n 13  authors_parsed  250000 non-null  object\ndtypes: object(14)\nmemory usage: 26.7+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"data_df = data_df[['id','title','authors','categories', 'abstract']]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:57.412453Z","iopub.execute_input":"2023-03-15T11:01:57.413125Z","iopub.status.idle":"2023-03-15T11:01:57.532644Z","shell.execute_reply.started":"2023-03-15T11:01:57.413085Z","shell.execute_reply":"2023-03-15T11:01:57.531239Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_df.to_csv(\"data_df.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:57.534436Z","iopub.execute_input":"2023-03-15T11:01:57.535793Z","iopub.status.idle":"2023-03-15T11:02:03.203341Z","shell.execute_reply.started":"2023-03-15T11:01:57.535748Z","shell.execute_reply":"2023-03-15T11:02:03.201724Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_df['abstract'][5]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.205304Z","iopub.execute_input":"2023-03-15T11:02:03.205967Z","iopub.status.idle":"2023-03-15T11:02:03.214176Z","shell.execute_reply.started":"2023-03-15T11:02:03.205921Z","shell.execute_reply":"2023-03-15T11:02:03.212561Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'  We study the two-particle wave function of paired atoms in a Fermi gas with\\ntunable interaction strengths controlled by Feshbach resonance. The Cooper pair\\nwave function is examined for its bosonic characters, which is quantified by\\nthe correction of Bose enhancement factor associated with the creation and\\nannihilation composite particle operators. An example is given for a\\nthree-dimensional uniform gas. Two definitions of Cooper pair wave function are\\nexamined. One of which is chosen to reflect the off-diagonal long range order\\n(ODLRO). Another one corresponds to a pair projection of a BCS state. On the\\nside with negative scattering length, we found that paired atoms described by\\nODLRO are more bosonic than the pair projected definition. It is also found\\nthat at $(k_F a)^{-1} \\\\ge 1$, both definitions give similar results, where more\\nthan 90% of the atoms occupy the corresponding molecular condensates.\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"#Function to decontract contractions\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.216164Z","iopub.execute_input":"2023-03-15T11:02:03.216680Z","iopub.status.idle":"2023-03-15T11:02:03.225673Z","shell.execute_reply.started":"2023-03-15T11:02:03.216634Z","shell.execute_reply":"2023-03-15T11:02:03.224465Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Function to remove all charaters between $ characters \ndef remove_eqns(txt):\n    reg = re.compile(r'\\$*?\\$') #Regex for a URL\n    return reg.sub(r'', txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.227162Z","iopub.execute_input":"2023-03-15T11:02:03.228201Z","iopub.status.idle":"2023-03-15T11:02:03.236612Z","shell.execute_reply.started":"2023-03-15T11:02:03.228155Z","shell.execute_reply":"2023-03-15T11:02:03.235284Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Function to replace all \\n characters with a space\ndef remove_newlines(txt):\n    return re.sub(r'\\n', \" \", txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.238088Z","iopub.execute_input":"2023-03-15T11:02:03.239049Z","iopub.status.idle":"2023-03-15T11:02:03.248350Z","shell.execute_reply.started":"2023-03-15T11:02:03.239011Z","shell.execute_reply":"2023-03-15T11:02:03.247451Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Function to remove all special characters from a text\ndef remove_spl(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.249562Z","iopub.execute_input":"2023-03-15T11:02:03.249881Z","iopub.status.idle":"2023-03-15T11:02:03.259296Z","shell.execute_reply.started":"2023-03-15T11:02:03.249849Z","shell.execute_reply":"2023-03-15T11:02:03.258197Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Function to remove stopwords from the text and lemmatize the words in the text\ndef remove_stopwords(txt):\n    words = word_tokenize(txt) #Tokenizing the text\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [word for word in words if word not in stop_words] #Removing stopwords and lemmatizing the words\n    filtered_txt = ' '.join(filtered_words) #Joining the filtered words back into a string\n    return filtered_txt","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.260807Z","iopub.execute_input":"2023-03-15T11:02:03.261868Z","iopub.status.idle":"2023-03-15T11:02:03.269881Z","shell.execute_reply.started":"2023-03-15T11:02:03.261821Z","shell.execute_reply":"2023-03-15T11:02:03.268814Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Function to remove stopwords from the text and lemmatize the words in the text\ndef remove_stopwords_lemmatize(txt):\n    words = word_tokenize(txt) #Tokenizing the text\n    lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words] #Removing stopwords and lemmatizing the words\n    filtered_txt = ' '.join(filtered_words) #Joining the filtered words back into a string\n    return filtered_txt","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.271329Z","iopub.execute_input":"2023-03-15T11:02:03.272002Z","iopub.status.idle":"2023-03-15T11:02:03.285259Z","shell.execute_reply.started":"2023-03-15T11:02:03.271965Z","shell.execute_reply":"2023-03-15T11:02:03.283843Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def preprocess(df_column):\n    df_column = df_column.apply(decontracted) \n    df_column = df_column.apply(remove_eqns)\n    df_column = df_column.apply(remove_newlines)\n    df_column = df_column.apply(remove_spl) \n    df_column = df_column.apply(lambda txt : txt.lower()) #Converting text to lowercase\n    df_column = df_column.apply(remove_stopwords_lemmatize)\n    return df_column","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.287199Z","iopub.execute_input":"2023-03-15T11:02:03.287762Z","iopub.status.idle":"2023-03-15T11:02:03.296033Z","shell.execute_reply.started":"2023-03-15T11:02:03.287716Z","shell.execute_reply":"2023-03-15T11:02:03.294959Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data_df['abstract'] = preprocess(data_df['abstract'])\ndata_df['title'] = preprocess(data_df['title'])\ndata_df['authors'] = preprocess(data_df['authors'])","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.297348Z","iopub.execute_input":"2023-03-15T11:02:03.297714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to replace all occurrences of . and - with _ in the given text.\"\"\"\ndef replace_chars(text):\n    return text.replace('.', '_').replace('-', '_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing categories\ndata_df['categories'] = data_df['categories'].apply(replace_chars)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['final_text'] = data_df['categories'] + \" \" + data_df['authors'] + \" \" + data_df['title'] + \" \" + data_df['abstract']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = data_df[['id','final_text']].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.to_csv(\"final_df.csv\",index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature extraction","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\n\n# Generate the tf-idf vectors for the data\ntfidf_matrix = tfidf_vectorizer.fit_transform(final_df['final_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO Doc to Vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting recommendations","metadata":{}},{"cell_type":"code","source":"cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix[0])\ns = cosine_sim.reshape(cosine_sim.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argsort(-s)[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[s[i] for i in np.argsort(-s)[1:6]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df['id'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_recommendations(paper_id:str,tfidf_matrix,num_rec):\n    idx = final_df.index[final_df['id'] == paper_id][0]\n    sim = cosine_similarity(tfidf_matrix, tfidf_matrix[idx])\n    sim = s`im.reshape(sim.shape[0])\n    top_n_idx = np.argsort(-sim)[1:num_rec+1]\n    top_n_id = [final_df['id'][x] for x in top_n_idx]\n    return top_n_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}