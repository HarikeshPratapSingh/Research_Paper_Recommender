{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n# data manipulation, gathering\nimport numpy as np # for array manipulation\nimport pandas as pd # for dataframe manipulation/reading in data\nimport json # for reading in Data\nfrom itertools import islice # for slicing and dicing JSON records\nimport os # for getting the filepath information\nimport re # to identify characters that are to be removed\nimport nltk # for preprocessing of textual data\nfrom nltk.corpus import stopwords # for removing stopwords\nfrom nltk.tokenize import word_tokenize # for tokenizing text\nfrom nltk.stem import WordNetLemmatizer # for lemmatizing text\nfrom sklearn.feature_extraction.text import TfidfVectorizer # for featurizing text\nfrom sklearn.metrics.pairwise import cosine_similarity # for getting similarity score\nfrom sklearn.decomposition import PCA #for dimensionality reduction\nfrom sklearn.cluster import KMeans #for clustering\nfrom sklearn.manifold import TSNE #For reducing to 2 dimensions for plotting","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:42.999223Z","iopub.execute_input":"2023-03-15T11:01:43.000567Z","iopub.status.idle":"2023-03-15T11:01:43.180583Z","shell.execute_reply.started":"2023-03-15T11:01:43.000511Z","shell.execute_reply":"2023-03-15T11:01:43.179191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:43.182248Z","iopub.execute_input":"2023-03-15T11:01:43.183201Z","iopub.status.idle":"2023-03-15T11:01:44.635622Z","shell.execute_reply.started":"2023-03-15T11:01:43.183149Z","shell.execute_reply":"2023-03-15T11:01:44.634582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the dataset","metadata":{}},{"cell_type":"code","source":"#Function to yield data from the stored file\ndef extract_data(path):\n    with open(path, 'r') as f:\n        for x in f:\n            yield x\n            \n#Defining PATH\nPATH = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\n\n\n#Creating a data generator to extract data from the JSON file\ndata_gen = extract_data(PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-15T11:01:44.638276Z","iopub.execute_input":"2023-03-15T11:01:44.638891Z","iopub.status.idle":"2023-03-15T11:01:44.646131Z","shell.execute_reply.started":"2023-03-15T11:01:44.638851Z","shell.execute_reply":"2023-03-15T11:01:44.644854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to yield N records from the data generator\ndef fetch_n_records(data_gen, n):\n    return [json.loads(record) for record in islice(data_gen, n)]\n\n#Fetching 250000 records from the given data for the use of recommender systems\nCHUNK_SIZE = 250000\ndata = fetch_n_records(data_gen, CHUNK_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:44.648162Z","iopub.execute_input":"2023-03-15T11:01:44.648738Z","iopub.status.idle":"2023-03-15T11:01:55.969720Z","shell.execute_reply.started":"2023-03-15T11:01:44.648692Z","shell.execute_reply":"2023-03-15T11:01:55.968403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The variable data records is a list of dictionaries\ndata[77]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:55.971203Z","iopub.execute_input":"2023-03-15T11:01:55.971641Z","iopub.status.idle":"2023-03-15T11:01:55.980009Z","shell.execute_reply.started":"2023-03-15T11:01:55.971601Z","shell.execute_reply":"2023-03-15T11:01:55.978714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to generate a dataframe from a list of dictionaries\ndef get_dataframe(list_of_dicts):\n    data = pd.DataFrame(list_of_dicts)\n    return data\n\n#Generating dataframe from the list of records\ndata_df = get_dataframe(data)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:55.994938Z","iopub.execute_input":"2023-03-15T11:01:55.995393Z","iopub.status.idle":"2023-03-15T11:01:56.872736Z","shell.execute_reply.started":"2023-03-15T11:01:55.995355Z","shell.execute_reply":"2023-03-15T11:01:56.871565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:56.874593Z","iopub.execute_input":"2023-03-15T11:01:56.875026Z","iopub.status.idle":"2023-03-15T11:01:57.406960Z","shell.execute_reply.started":"2023-03-15T11:01:56.874980Z","shell.execute_reply":"2023-03-15T11:01:57.405589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = data_df[['id','title','authors','categories', 'abstract']]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:57.412453Z","iopub.execute_input":"2023-03-15T11:01:57.413125Z","iopub.status.idle":"2023-03-15T11:01:57.532644Z","shell.execute_reply.started":"2023-03-15T11:01:57.413085Z","shell.execute_reply":"2023-03-15T11:01:57.531239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.to_csv(\"data_df.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:01:57.534436Z","iopub.execute_input":"2023-03-15T11:01:57.535793Z","iopub.status.idle":"2023-03-15T11:02:03.203341Z","shell.execute_reply.started":"2023-03-15T11:01:57.535748Z","shell.execute_reply":"2023-03-15T11:02:03.201724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['abstract'][5]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.205304Z","iopub.execute_input":"2023-03-15T11:02:03.205967Z","iopub.status.idle":"2023-03-15T11:02:03.214176Z","shell.execute_reply.started":"2023-03-15T11:02:03.205921Z","shell.execute_reply":"2023-03-15T11:02:03.212561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"#Function to decontract contractions\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.216164Z","iopub.execute_input":"2023-03-15T11:02:03.216680Z","iopub.status.idle":"2023-03-15T11:02:03.225673Z","shell.execute_reply.started":"2023-03-15T11:02:03.216634Z","shell.execute_reply":"2023-03-15T11:02:03.224465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to remove all charaters between $ characters \ndef remove_eqns(txt):\n    reg = re.compile(r'\\$*?\\$') #Regex for a URL\n    return reg.sub(r'', txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.227162Z","iopub.execute_input":"2023-03-15T11:02:03.228201Z","iopub.status.idle":"2023-03-15T11:02:03.236612Z","shell.execute_reply.started":"2023-03-15T11:02:03.228155Z","shell.execute_reply":"2023-03-15T11:02:03.235284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to replace all \\n characters with a space\ndef remove_newlines(txt):\n    return re.sub(r'\\n', \" \", txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.238088Z","iopub.execute_input":"2023-03-15T11:02:03.239049Z","iopub.status.idle":"2023-03-15T11:02:03.248350Z","shell.execute_reply.started":"2023-03-15T11:02:03.239011Z","shell.execute_reply":"2023-03-15T11:02:03.247451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to remove all special characters from a text\ndef remove_spl(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.249562Z","iopub.execute_input":"2023-03-15T11:02:03.249881Z","iopub.status.idle":"2023-03-15T11:02:03.259296Z","shell.execute_reply.started":"2023-03-15T11:02:03.249849Z","shell.execute_reply":"2023-03-15T11:02:03.258197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to remove stopwords from the text and lemmatize the words in the text\ndef remove_stopwords(txt):\n    words = word_tokenize(txt) #Tokenizing the text\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [word for word in words if word not in stop_words] #Removing stopwords and lemmatizing the words\n    filtered_txt = ' '.join(filtered_words) #Joining the filtered words back into a string\n    return filtered_txt","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.260807Z","iopub.execute_input":"2023-03-15T11:02:03.261868Z","iopub.status.idle":"2023-03-15T11:02:03.269881Z","shell.execute_reply.started":"2023-03-15T11:02:03.261821Z","shell.execute_reply":"2023-03-15T11:02:03.268814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to remove stopwords from the text and lemmatize the words in the text\ndef remove_stopwords_lemmatize(txt):\n    words = word_tokenize(txt) #Tokenizing the text\n    lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words] #Removing stopwords and lemmatizing the words\n    filtered_txt = ' '.join(filtered_words) #Joining the filtered words back into a string\n    return filtered_txt","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.271329Z","iopub.execute_input":"2023-03-15T11:02:03.272002Z","iopub.status.idle":"2023-03-15T11:02:03.285259Z","shell.execute_reply.started":"2023-03-15T11:02:03.271965Z","shell.execute_reply":"2023-03-15T11:02:03.283843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df_column):\n    df_column = df_column.apply(decontracted) \n    df_column = df_column.apply(remove_eqns)\n    df_column = df_column.apply(remove_newlines)\n    df_column = df_column.apply(remove_spl) \n    df_column = df_column.apply(lambda txt : txt.lower()) #Converting text to lowercase\n    df_column = df_column.apply(remove_stopwords_lemmatize)\n    return df_column","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.287199Z","iopub.execute_input":"2023-03-15T11:02:03.287762Z","iopub.status.idle":"2023-03-15T11:02:03.296033Z","shell.execute_reply.started":"2023-03-15T11:02:03.287716Z","shell.execute_reply":"2023-03-15T11:02:03.294959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['abstract'] = preprocess(data_df['abstract'])\ndata_df['title'] = preprocess(data_df['title'])\ndata_df['authors'] = preprocess(data_df['authors'])","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:02:03.297348Z","iopub.execute_input":"2023-03-15T11:02:03.297714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to replace all occurrences of . and - with _ in the given text.\"\"\"\ndef replace_chars(text):\n    return text.replace('.', '_').replace('-', '_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing categories\ndata_df['categories'] = data_df['categories'].apply(replace_chars)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['final_text'] = data_df['categories'] + \" \" + data_df['authors'] + \" \" + data_df['title'] + \" \" + data_df['abstract']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = data_df[['id','final_text']].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.to_csv(\"final_df.csv\",index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature extraction","metadata":{}},{"cell_type":"code","source":"initial_df = pd.read_csv(\"/kaggle/input/research-paper-data/initial_df\")\nfinal_df = pd.read_csv(\"/kaggle/input/research-paper-data/final_df\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:05:00.789360Z","iopub.execute_input":"2023-05-06T05:05:00.789678Z","iopub.status.idle":"2023-05-06T05:05:10.956517Z","shell.execute_reply.started":"2023-05-06T05:05:00.789646Z","shell.execute_reply":"2023-05-06T05:05:10.955259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\n\n# Generate the tf-idf vectors for the data\ntfidf_matrix = tfidf_vectorizer.fit_transform(final_df['final_text'])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:05.198467Z","iopub.execute_input":"2023-05-06T05:06:05.198880Z","iopub.status.idle":"2023-05-06T05:06:27.406306Z","shell.execute_reply.started":"2023-05-06T05:06:05.198841Z","shell.execute_reply":"2023-05-06T05:06:27.405223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:27:16.317806Z","iopub.execute_input":"2023-05-05T19:27:16.318125Z","iopub.status.idle":"2023-05-05T19:27:16.325542Z","shell.execute_reply.started":"2023-05-05T19:27:16.318094Z","shell.execute_reply":"2023-05-05T19:27:16.324568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting recommendations","metadata":{}},{"cell_type":"code","source":"cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix[0])\ns = cosine_sim.reshape(cosine_sim.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:07:56.000151Z","iopub.execute_input":"2023-05-05T19:07:56.000577Z","iopub.status.idle":"2023-05-05T19:07:56.246988Z","shell.execute_reply.started":"2023-05-05T19:07:56.000540Z","shell.execute_reply":"2023-05-05T19:07:56.246029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argsort(-s)[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[s[i] for i in np.argsort(-s)[1:6]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df['id'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_recommendations(paper_id:str,tfidf_matrix,num_rec):\n    idx = final_df.index[final_df['id'] == paper_id][0]\n    sim = cosine_similarity(tfidf_matrix, tfidf_matrix[idx])\n    sim = sim.reshape(sim.shape[0])\n    top_n_idx = np.argsort(-sim)[1:num_rec+1]\n    top_n_id = [final_df['id'][x] for x in top_n_idx]\n    return top_n_id","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:27.408192Z","iopub.execute_input":"2023-05-06T05:06:27.408498Z","iopub.status.idle":"2023-05-06T05:06:27.415934Z","shell.execute_reply.started":"2023-05-06T05:06:27.408468Z","shell.execute_reply":"2023-05-06T05:06:27.414737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer2 = TfidfVectorizer(max_features=10000)\n\n# Generate the tf-idf vectors for the data\ntfidf_matrix2 = tfidf_vectorizer2.fit_transform(final_df['final_text'])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:27.417499Z","iopub.execute_input":"2023-05-06T05:06:27.418326Z","iopub.status.idle":"2023-05-06T05:06:49.117041Z","shell.execute_reply.started":"2023-05-06T05:06:27.418279Z","shell.execute_reply":"2023-05-06T05:06:49.116103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec = get_recommendations(704.0001,tfidf_matrix,1000)\nidxs = list(final_df[final_df['id'].isin(rec)].index)\nrec_matrix = tfidf_matrix2[idxs]","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:49.119112Z","iopub.execute_input":"2023-05-06T05:06:49.119915Z","iopub.status.idle":"2023-05-06T05:06:49.417112Z","shell.execute_reply.started":"2023-05-06T05:06:49.119868Z","shell.execute_reply":"2023-05-06T05:06:49.416046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=0.95, random_state=42) #Keep 95% of the variance\nreduced_matrix = pca.fit_transform(rec_matrix.toarray())","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:49.418417Z","iopub.execute_input":"2023-05-06T05:06:49.418717Z","iopub.status.idle":"2023-05-06T05:06:52.721353Z","shell.execute_reply.started":"2023-05-06T05:06:49.418686Z","shell.execute_reply":"2023-05-06T05:06:52.719586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:52.723456Z","iopub.execute_input":"2023-05-06T05:06:52.724301Z","iopub.status.idle":"2023-05-06T05:06:52.733965Z","shell.execute_reply.started":"2023-05-06T05:06:52.724253Z","shell.execute_reply":"2023-05-06T05:06:52.732479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 10 # selectable\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(reduced_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:52.736149Z","iopub.execute_input":"2023-05-06T05:06:52.736975Z","iopub.status.idle":"2023-05-06T05:06:54.554803Z","shell.execute_reply.started":"2023-05-06T05:06:52.736927Z","shell.execute_reply":"2023-05-06T05:06:54.553676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(perplexity=100, random_state=42)\ntwo_dim_matrix = tsne.fit_transform(reduced_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:06:54.556202Z","iopub.execute_input":"2023-05-06T05:06:54.556548Z","iopub.status.idle":"2023-05-06T05:07:01.736849Z","shell.execute_reply.started":"2023-05-06T05:06:54.556515Z","shell.execute_reply":"2023-05-06T05:07:01.735870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(rc={'figure.figsize':(5,5)})\n\n\n# plot\nsns.scatterplot(x=two_dim_matrix[:,0], y=two_dim_matrix[:,1], hue=y_pred, legend='full', palette=\"Set1\")\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"cluster_tsne.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:07:01.738588Z","iopub.execute_input":"2023-05-06T05:07:01.739281Z","iopub.status.idle":"2023-05-06T05:07:02.874236Z","shell.execute_reply.started":"2023-05-06T05:07:01.739244Z","shell.execute_reply":"2023-05-06T05:07:02.873171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(initial_df[final_df['id'].isin(rec)], x=two_dim_matrix[:,0], y=two_dim_matrix[:,1], color=y_pred.astype(str),\n                 hover_data=['id','title'],\n                 height= 525, width=525,\n                title = \"Clustered Papers\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:07:02.877620Z","iopub.execute_input":"2023-05-06T05:07:02.878849Z","iopub.status.idle":"2023-05-06T05:07:06.604241Z","shell.execute_reply.started":"2023-05-06T05:07:02.878798Z","shell.execute_reply":"2023-05-06T05:07:06.603050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Topic Modelling For Keyword Extraction","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:45:59.160303Z","iopub.execute_input":"2023-05-06T05:45:59.161432Z","iopub.status.idle":"2023-05-06T05:45:59.166396Z","shell.execute_reply.started":"2023-05-06T05:45:59.161388Z","shell.execute_reply":"2023-05-06T05:45:59.165277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizers = []\n    \nfor x in range(0, k):\n    # Creating a vectorizer\n    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:45:59.615416Z","iopub.execute_input":"2023-05-06T05:45:59.616464Z","iopub.status.idle":"2023-05-06T05:45:59.622281Z","shell.execute_reply.started":"2023-05-06T05:45:59.616419Z","shell.execute_reply":"2023-05-06T05:45:59.620882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_df = pd.DataFrame()\ntopic_df['id'] = initial_df[final_df['id'].isin(rec)]['id']\ntopic_df['title'] = initial_df[final_df['id'].isin(rec)]['title']\ntopic_df['text'] = initial_df[final_df['id'].isin(rec)]['title']+\" \"+initial_df[final_df['id'].isin(rec)]['abstract']\ntopic_df['cluster'] = y_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:06:42.376811Z","iopub.execute_input":"2023-05-06T06:06:42.377731Z","iopub.status.idle":"2023-05-06T06:06:42.423302Z","shell.execute_reply.started":"2023-05-06T06:06:42.377684Z","shell.execute_reply":"2023-05-06T06:06:42.422356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorized_data = []\n\nfor current_cluster, cvec in enumerate(vectorizers):\n    try:\n        vectorized_data.append(cvec.fit_transform(topic_df.loc[topic_df['cluster'] == current_cluster, 'text']))\n    except Exception as e:\n        print(\"Not enough instances in cluster: \" + str(current_cluster))\n        vectorized_data.append(None)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:46:01.777627Z","iopub.execute_input":"2023-05-06T05:46:01.778210Z","iopub.status.idle":"2023-05-06T05:46:01.878579Z","shell.execute_reply.started":"2023-05-06T05:46:01.778167Z","shell.execute_reply":"2023-05-06T05:46:01.877414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TOPICS_PER_CLUSTER = 5 #choose\n\nlda_models = []\nfor x in range(0, k):\n    # Latent Dirichlet Allocation Model\n    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n    lda_models.append(lda)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:46:02.678981Z","iopub.execute_input":"2023-05-06T05:46:02.679407Z","iopub.status.idle":"2023-05-06T05:46:02.685509Z","shell.execute_reply.started":"2023-05-06T05:46:02.679366Z","shell.execute_reply":"2023-05-06T05:46:02.684189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_lda_data = []\n\nfor current_cluster, lda in enumerate(lda_models):\n    #print(\"Current Cluster: \" + str(current_cluster))\n    \n    if vectorized_data[current_cluster] != None:\n        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:46:05.874400Z","iopub.execute_input":"2023-05-06T05:46:05.874824Z","iopub.status.idle":"2023-05-06T05:46:10.201677Z","shell.execute_reply.started":"2023-05-06T05:46:05.874783Z","shell.execute_reply":"2023-05-06T05:46:10.200129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def selected_topics(model, vectorizer, top_n=3):\n    current_words = []\n    keywords = []\n    \n    for idx, topic in enumerate(model.components_):\n        words = [(vectorizer.get_feature_names_out()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n        for word in words:\n            if word[0] not in current_words:\n                keywords.append(word)\n                current_words.append(word[0])\n                \n    keywords.sort(key = lambda x: x[1])  \n    keywords.reverse()\n    return_values = []\n    for x in keywords:\n        return_values.append(x[0])\n    return \" \".join(return_values)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:52:33.781693Z","iopub.execute_input":"2023-05-06T05:52:33.782160Z","iopub.status.idle":"2023-05-06T05:52:33.790243Z","shell.execute_reply.started":"2023-05-06T05:52:33.782119Z","shell.execute_reply":"2023-05-06T05:52:33.789275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_keywords = []\nfor current_vectorizer, lda in enumerate(lda_models):\n    #print(\"Current Cluster: \" + str(current_vectorizer))\n\n    if vectorized_data[current_vectorizer] != None:\n        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:52:42.234923Z","iopub.execute_input":"2023-05-06T05:52:42.235945Z","iopub.status.idle":"2023-05-06T05:52:42.256620Z","shell.execute_reply.started":"2023-05-06T05:52:42.235904Z","shell.execute_reply":"2023-05-06T05:52:42.255476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_keyword = {x:all_keywords[x] for x in range(k)}\nword_pred = list(map(cluster_keyword.get, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:52:46.014544Z","iopub.execute_input":"2023-05-06T05:52:46.014953Z","iopub.status.idle":"2023-05-06T05:52:46.022040Z","shell.execute_reply.started":"2023-05-06T05:52:46.014917Z","shell.execute_reply":"2023-05-06T05:52:46.021030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_df['keywords'] = word_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:06:57.286782Z","iopub.execute_input":"2023-05-06T06:06:57.287244Z","iopub.status.idle":"2023-05-06T06:06:57.293636Z","shell.execute_reply.started":"2023-05-06T06:06:57.287189Z","shell.execute_reply":"2023-05-06T06:06:57.292435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_df['link'] = ","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:08:19.417195Z","iopub.execute_input":"2023-05-06T06:08:19.418281Z","iopub.status.idle":"2023-05-06T06:08:19.434113Z","shell.execute_reply.started":"2023-05-06T06:08:19.418237Z","shell.execute_reply":"2023-05-06T06:08:19.432934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(topic_df, x=two_dim_matrix[:,0], y=two_dim_matrix[:,1], color='keywords',\n                 hover_data=['id','title'],\n                 height= 500, width=1200,\n                title = \"Clustered Papers\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:07:01.855651Z","iopub.execute_input":"2023-05-06T06:07:01.856464Z","iopub.status.idle":"2023-05-06T06:07:01.970353Z","shell.execute_reply.started":"2023-05-06T06:07:01.856419Z","shell.execute_reply":"2023-05-06T06:07:01.969500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig.write_html(\"plot.html\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:58:32.287662Z","iopub.execute_input":"2023-05-06T05:58:32.288091Z","iopub.status.idle":"2023-05-06T05:58:32.318495Z","shell.execute_reply.started":"2023-05-06T05:58:32.288042Z","shell.execute_reply":"2023-05-06T05:58:32.316990Z"},"trusted":true},"execution_count":null,"outputs":[]}]}